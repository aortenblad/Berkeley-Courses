---
title: "HW4 - Final"
output:
  html_document: default
  pdf_document: default
---
```{r, set.seed(1)}
library(ggplot2)
knitr::opts_chunk$set(cache = TRUE)
```

###Exercise 4F
```{r}
r = runif(1, 2, 4)
lambda = runif(1, 1, 2)
gamma_sample = data.frame(x = rgamma(200, shape = r, scale = lambda))
lambda
r
ggplot(gamma_sample, aes(x=gamma_sample$x)) + 
      geom_histogram(binwidth = 2, aes(y=stat(density))) +
      stat_function(fun=dgamma, args=list(shape=r,scale=lambda), col='red') + 
     labs(title = "Histogram of sample size n = 200", x= "my_data", y="count")
```

###Exercise 4G
```{r}
sample_mean = mean(gamma_sample$x)
sample_variance = mean((gamma_sample$x)^2) - (mean(gamma_sample$x))^2
r_MOM = (sample_mean)^2/(sample_variance)
lambda_MOM = sample_mean/sample_variance
lambda_MOM
r_MOM
#generate 1000 samples of size 200 with lambda_MOM and r_MOM parameters
samples_1000 = replicate(1000, rgamma(200, shape = r_MOM, scale = lambda_MOM))
#find 1000 r and lambda using MOM for each sample
sampled_r = c()
sampled_lambda = c()
for(i in c(1:1000)){
  this_mean = mean(samples_1000[,i])
  this_variance = mean((samples_1000[,i])^2) - (mean(samples_1000[,i]))^2
  this_r = (this_mean)^2/(this_variance)
  this_lambda = this_mean/this_variance
  sampled_r = c(sampled_r, this_r)
  sampled_lambda = c(sampled_lambda, this_lambda)
}

#plot the histograms
df = data.frame(sampled_r, sampled_lambda)
sampled_r_hist = ggplot(df, aes(x=df$sampled_r)) +
                 geom_histogram(binwidth = 0.08, aes(y=stat(density))) +
                 geom_vline(xintercept = r, col = "red") + 
                 labs(title = "Bootstrapped r_hat", x="r_hat" , y="count")

sampled_lambda_hist = ggplot(df, aes(x=df$sampled_lambda)) +
                 geom_histogram(binwidth = 0.05, aes(y=stat(density))) +
                 geom_vline(xintercept = lambda, col = "blue") + 
                 labs(title = "Bootstrapped lambda_hat", x="lambda_hat" , y="count")

#Estimate standard erros
SE_r = sd(df$sampled_r)
SE_lambda = sd(df$sampled_lambda)

SE_r
SE_lambda
sampled_r_hist
sampled_lambda_hist
```


The histograms are both somewhat similar to those in figure 8.4 in that they seem somewhat symmetric and centered around the MOM estimates. None of the true values of r and lambda seem to be in a particularly surpissing place, since they are both somewhat close to the mean of the distributions.  

###Exercise 4H
```{r}
r_5th = quantile(gamma_sample$x, 0.05)
r_95th = quantile(gamma_sample$x, 0.95)
r_5th
r_95th

ggplot(gamma_sample, aes(x)) + 
  geom_histogram(binwidth = 1, aes(y=stat(density))) +
  stat_function(fun=dgamma, args=list(shape=r,scale=lambda), col='red') + 
  stat_function(fun=dgamma, args=list(shape=r_5th,scale=lambda_MOM), col='blue') +
  stat_function(fun=dgamma, args=list(shape=r_95th,scale=lambda_MOM), col='green') + 
  labs(title = "Original sample with different gamma densities", x="my_data" , y="count")
```


It is not really surprissing that the gamma density with the MOM estimators (red line) provides the best fit, since the other two estimators (blue is 95% and green 5%) are, by construction, heavily skewed away from the true parameter value. 

###Exercise 4I 
```{r}
bootstrap_resamples = replicate(1000, sample(gamma_sample$x, size=200, replace = TRUE))
bootstraped_r = c()
bootstraped_lambda = c()
for(i in c(1:1000)){
  resample_mean = mean(bootstrap_resamples[,i])
  resample_variance = mean((bootstrap_resamples[,i])^2) - (mean(bootstrap_resamples[,i]))^2
  resample_r = (resample_mean)^2/(resample_variance)
  resample_lambda = resample_mean/resample_variance
  bootstraped_r = c(bootstraped_r, resample_r)
  bootstraped_lambda = c(bootstraped_lambda, resample_lambda)
}

#plot the histograms
bootstrapped_parameters = data.frame(bootstraped_r, bootstraped_lambda)
bootstraped_r_hist = ggplot(bootstrapped_parameters,         aes(x=bootstrapped_parameters$bootstraped_r)) + 
                  geom_histogram(binwidth = 0.08, aes(y=stat(density))) +
                  geom_vline(xintercept = r, col = "red") + 
                  labs(title = "Nonparametric bootstrap of r_hat", x="r_hat" , y="count")

bootstraped_lambda_hist = ggplot(bootstrapped_parameters, aes(x=bootstrapped_parameters$bootstraped_lambda)) + 
                  geom_histogram(binwidth = 0.03, aes(y=stat(density))) +                   geom_vline(xintercept = lambda, col = "blue") + 
                  labs(title = "Nonparametric bootstrap of lambda_hat", x="lambda_hat", y="count")  

#Estimate standard erros
SE_r_nonpara = sd(bootstrapped_parameters$bootstraped_r)
SE_lambda_nonpara = sd(bootstrapped_parameters$bootstraped_lambda)

SE_r_nonpara
SE_lambda_nonpara
bootstraped_r_hist
bootstraped_lambda_hist

```  


Though these distribution are similar to those in 4H in the sense that they are symmetric, it is clear that the distribution of lambda is far less predictive in the nonparametric case, as the true value is far from the distribution. 

###Exercise 4J
```{r}
#Create function to optimize with Newton's Method
fun = function(para, x) {
  r = para[1]
  lambda = para[2]
  -sum(r * log(lambda) + (r - 1)*log(x) - lambda * x - log(gamma(r)))
}

mle = optim(par = c(r_MOM, lambda_MOM), fn = fun, x=gamma_sample$x)

r_mle = mle$par[1]
lambda_mle = mle$par[2]
r_mle
lambda_mle
```  
The MLE estimates seem to both be good predictors of the true parameter, since r_MLE is 3.52 while the true r is 3.29. Likewise lambda_MLE is 0.906 while the true lambda is 1.40